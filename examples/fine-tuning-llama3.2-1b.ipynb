{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195c92e24104eb98b3ca7cab5c49854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 750\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "num_training_samples = 15000\n",
    "num_test_samples = 750\n",
    "num_validation_samples = 1000\n",
    "training_samples = squad['train'].select([i for i in range(num_training_samples)])\n",
    "test_samples = squad['train'].select([i for i in range(num_training_samples, num_training_samples+num_test_samples)])\n",
    "validation_samples = squad['validation'].select([i for i in range(num_validation_samples)])\n",
    "print(training_samples)\n",
    "print(test_samples)\n",
    "print(validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf964f487c9494fa31009003dfee198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d4a5362cca4a759e4fea1afdc3ff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb520fa2c744d2f90f25b6920ea4697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly set the chat template clearly:\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'system' %}<|system|>\\n{{ message['content'] }}\\n\"\n",
    "    \"{% elif message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>{{ message['content'] }}<|eot_id|>\"\n",
    "    \"{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>{{ message['content'] }}<|eot_id|>\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_squad_sample_to_llama_conversation(sample):\n",
    "    question = sample['question']\n",
    "    context = sample['context']\n",
    "\n",
    "    answers = sample['answers']['text']\n",
    "    if len(answers) == 0:\n",
    "        answer = \"The context does not provide an answer...\"\n",
    "    else:\n",
    "        answer = answers[0]\n",
    "\n",
    "    instruction_prompt_template = '''\n",
    "    You are a helpful assistant tasked with extracting exact passages from the context that answer the user's questions. \n",
    "    Output exact passages word for word from the context. If the answer isn't found, reply \"The context does not provide an answer...\".\n",
    "\n",
    "    Context: {context}\n",
    "    '''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction_prompt_template.format(context=context)},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "\n",
    "    # Tokenize the entire conversation explicitly as text\n",
    "    conversation_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    # Tokenize the conversation text\n",
    "    tokenized_output = tokenizer(\n",
    "        conversation_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    labels = tokenized_output['input_ids'].clone()\n",
    "\n",
    "    # Prepare prompt text explicitly for masking\n",
    "    prompt_messages = messages[:-1]  # exclude assistant's response\n",
    "    prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "    prompt_tokens = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # Explicitly mask the prompt tokens in the labels\n",
    "    labels[:, :len(prompt_tokens)] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_output[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": tokenized_output[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": labels.squeeze()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5192bb6cf046d581c857204c396e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fe72d475494d6aa336b2a7ea64cda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e4ba898e84524bfc24b3970ba86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = training_samples.map(\n",
    "    convert_squad_sample_to_llama_conversation,\n",
    "    remove_columns=training_samples.column_names\n",
    ")\n",
    "\n",
    "tokenized_validation_dataset = validation_samples.map(\n",
    "    convert_squad_sample_to_llama_conversation, \n",
    "    remove_columns=validation_samples.column_names\n",
    "    )\n",
    "\n",
    "tokenized_test_dataset = test_samples.map(\n",
    "    convert_squad_sample_to_llama_conversation,\n",
    "    remove_columns=test_samples.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d4c1837e7c43a688699343152f7485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da052ad7dd64a34a81bbaab6ecb65fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d2b2d6e3ba4b5aa50c16f86b39238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# to help save on gpu space and run this a bit faster we'll load the model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "# rank defines the rank of the adapter matrix,\n",
    "# the higher the rank, the more complex the task it's trying to learn\n",
    "rank = 128\n",
    "\n",
    "# the alpha is a scaling factor hyper parameter, basically controls how much our\n",
    "# adapter will influence the models output, the higher this value\n",
    "# the more our adapter will overpower the original model weights.\n",
    "# there is a lot of advice out there for what the alpha value should be\n",
    "# keeping the alpha at around 2x of what the rank is works for this notebook\n",
    "alpha = rank*2\n",
    "peft_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    lora_dropout=0.05, # dropout for the lora layers while training, to avoid overfitting\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # the target modules defines what types of layers to add lora adapters too, so in the network\n",
    "    # any model that have a name in this list will have a lora adapter added to it,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafaac5016f74468a5c76159e1a3d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b03167aa63469ba169d400574ac810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef14fabf3c145efa9ce9d577a31e86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23631d13ee7a445fbef5d8809af65314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db87b3815e74682bdfc9a89df491b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15c1d45414c47d78bad3e698c094d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_checkpoint_path = \"./results/llama3.2-1b\"\n",
    "\n",
    "# an important note is that the loss function isn't defined here,\n",
    "# it's instead stored as a model parameter for models in hf,\n",
    "# in the case of llama it is cross entropy loss\n",
    "\n",
    "# first define some training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=model_checkpoint_path,\n",
    "    optim='adafactor', #specify what optimizer we wwant to use, in this case a 8bit version of adamw with pagination.\n",
    "    per_device_train_batch_size=8, # define the number of samples per training batch\n",
    "    gradient_accumulation_steps=4, # define how many steps to accumulate gradients,\n",
    "    log_level='debug',\n",
    "    eval_strategy = \"steps\",\n",
    "    save_strategy='steps', # we'll save a checkpoint every epoch\n",
    "    logging_steps=8,\n",
    "    eval_steps=8,\n",
    "    save_steps=8,\n",
    "    learning_rate=1e-5, # for llm training we want a fairly high learning rate, 1e-4 is a good starting point but it's worth it to play around with this value\n",
    "    fp16=True,\n",
    "    num_train_epochs=4,\n",
    "    max_steps=120,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end = True,\n",
    "    overwrite_output_dir = True,\n",
    "    lr_scheduler_type='linear',# and set our learning rate decay\n",
    ")\n",
    "\n",
    "# now that we have our arguments, we'll use that to create our trainer,\n",
    "# passing in the model, dataset, peft config, tokenizer, ect\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 90,177,536 || all params: 1,325,991,936 || trainable%: 6.8008\n"
     ]
    }
   ],
   "source": [
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 08:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4902470111846924, 'eval_model_preparation_time': 0.0018, 'eval_runtime': 77.3195, 'eval_samples_per_second': 12.933, 'eval_steps_per_second': 1.617}\n"
     ]
    }
   ],
   "source": [
    "initial_eval_values = trainer.evaluate()\n",
    "print(initial_eval_values)\n",
    "initial_eval_loss = initial_eval_values['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 15,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 120\n",
      "  Number of trainable parameters = 90,177,536\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 1:50:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.410900</td>\n",
       "      <td>3.096341</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.795700</td>\n",
       "      <td>2.276266</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.202500</td>\n",
       "      <td>1.950679</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.060400</td>\n",
       "      <td>1.873520</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.966300</td>\n",
       "      <td>1.833553</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.952700</td>\n",
       "      <td>1.811619</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.932500</td>\n",
       "      <td>1.798520</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.891500</td>\n",
       "      <td>1.789124</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.880300</td>\n",
       "      <td>1.781949</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.834600</td>\n",
       "      <td>1.777381</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.919700</td>\n",
       "      <td>1.772304</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.859000</td>\n",
       "      <td>1.769359</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.863600</td>\n",
       "      <td>1.767014</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.867100</td>\n",
       "      <td>1.766255</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.877300</td>\n",
       "      <td>1.765769</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-8\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-8/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-8/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-16\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-16/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-24\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-24/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-32\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-32/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-40\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-40/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-48\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-48/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-56\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-56/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-56/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-64\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-64/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-72\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-72/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-72/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-80\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-80/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-88\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-88/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-96\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-96/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-104\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-104/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-112\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-112/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/llama3.2-1b/checkpoint-120\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results/llama3.2-1b/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./results/llama3.2-1b/checkpoint-120/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/llama3.2-1b/checkpoint-120 (score: 1.7657690048217773).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=2.087611111005147, metrics={'train_runtime': 6653.2949, 'train_samples_per_second': 0.577, 'train_steps_per_second': 0.018, 'total_flos': 2.508695229431808e+16, 'train_loss': 2.087611111005147})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/llama3.2-1b\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./models/llama3.2-1b/tokenizer_config.json\n",
      "Special tokens file saved in ./models/llama3.2-1b/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./models/llama3.2-1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4109, 2.7957, 2.2025, 2.0604, 1.9663, 1.9527, 1.9325, 1.8915, 1.8803, 1.8346, 1.9197, 1.859, 1.8636, 1.8671, 1.8773]\n",
      "[3.4902470111846924, 3.0963408946990967, 2.2762656211853027, 1.950678825378418, 1.8735195398330688, 1.833552598953247, 1.811618685722351, 1.7985199689865112, 1.7891242504119873, 1.7819489240646362, 1.7773808240890503, 1.7723044157028198, 1.7693591117858887, 1.7670135498046875, 1.7662547826766968, 1.7657690048217773]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss_history = []\n",
    "eval_loss_history = [initial_eval_loss]\n",
    "for step in trainer.state.log_history:\n",
    "  if 'loss' in step:\n",
    "    training_loss_history.append(step['loss'])\n",
    "  elif \"eval_loss\" in step:\n",
    "    eval_loss_history.append(step['eval_loss'])\n",
    "\n",
    "print(training_loss_history)\n",
    "print(eval_loss_history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "time_steps = [i*16 for i in range(1, len(training_loss_history)+1)]\n",
    "plt.plot(time_steps, training_loss_history, label=\"train loss\")\n",
    "plt.plot([0]+time_steps, eval_loss_history, label=\"eval loss\")\n",
    "plt.title(\"Train and Eval Loss During Training\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./models/llama3.2-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/hans/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular? 1998\n",
      "When did Beyonce start becoming popular? 1998\n",
      "Post by Cyn » Thu Oct 31, 2013 11:59 pm\n",
      "When did Beyonce start becoming popular? 1998\n",
      "Beyoncé Knowles was born on January 4, 1981 in Houston, Texas. She is the daughter of Tina Knowles, who is an interior designer, and Mathew Knowles, who is a record producer. Her mother is African American, and her father is African American and of German and Scottish descent. Beyoncé has two younger sisters, Solange Knowles and Kelly Rowland. Beyoncé has a younger half-brother, Daniel Matthews, from her father's previous marriage. Beyoncé's paternal grandparents were a white German woman and a black African man. Her maternal grandparents are African American.\n",
      "Beyoncé's father, Mathew Knowles, was a record producer and manager, and her mother, Tina Knowles, was an interior designer. Beyoncé started singing at the age of four and was a member of the singing group Destiny's Child from 1990 to 2003. The group had a number of hit singles and albums and was one of the\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "trained_adapter_dir = modelpath  # your checkpoint folder clearly stated here\n",
    "\n",
    "# Load base tokenizer explicitly\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Clearly load base model explicitly\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Explicitly load your trained LoRA adapters clearly\n",
    "model = PeftModel.from_pretrained(base_model, trained_adapter_dir)\n",
    "\n",
    "# Set tokenizer explicitly\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Use model explicitly for inference clearly\n",
    "prompt = \"When did Beyonce start becoming popular?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=256)\n",
    "\n",
    "response = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
