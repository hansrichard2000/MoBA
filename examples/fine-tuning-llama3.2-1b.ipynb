{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa1c63fface42ff9b6a964fc75483e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 750\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "num_training_samples = 15000\n",
    "num_test_samples = 750\n",
    "num_validation_samples = 1000\n",
    "training_samples = squad['train'].select([i for i in range(num_training_samples)])\n",
    "test_samples = squad['train'].select([i for i in range(num_training_samples, num_training_samples+num_test_samples)])\n",
    "validation_samples = squad['validation'].select([i for i in range(num_validation_samples)])\n",
    "print(training_samples)\n",
    "print(test_samples)\n",
    "print(validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'BeyoncÃ©',\n",
       " 'context': 'BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57494a1c7e4145caa41b3f041628aa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f33ca9ba9a74d97b69fd534c599130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb33a5763304bceb8ee7d70f3c33c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa260a81cb94c638c74d71fb4f19c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly set the chat template clearly:\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'system' %}<|system|>\\n{{ message['content'] }}\\n\"\n",
    "    \"{% elif message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>{{ message['content'] }}<|eot_id|>\"\n",
    "    \"{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>{{ message['content'] }}<|eot_id|>\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_squad_sample_to_llama_conversation(sample):\n",
    "    # get the question and context for this sample\n",
    "    question = sample['question']\n",
    "    context = sample['context']\n",
    "\n",
    "    # some questions can have multiple answers, some none at all,\n",
    "    # for the case of no answers we'll have the model output that the\n",
    "    # context does not provide an answer, if it has multiple we'll just take\n",
    "    # the first answer as the ground truth.\n",
    "    answers = sample['answers']['text']\n",
    "    if len(answers) == 0 :\n",
    "      answer = \"The context does not provide an answer...\"\n",
    "    else:\n",
    "      answer = sample['answers']['text'][0]\n",
    "\n",
    "    # now we define an initial model prompt defining the task and giving the model the context passage\n",
    "    instruction_prompt_template = '''\n",
    "    You are a helpful assistant tasked with extracting passages that answer users questions from a given context. Output exact passages word for word that answer the users question. Do not output any other text other than passages in the context passage. Output the minimal amount to answer the question, for example only 2-3 words from the passage. If you cannot find the answer in the context passage output 'The context does not provide an answer...'\n",
    "\n",
    "    Context: {context}'''\n",
    "\n",
    "    # now we'll convert these into a list of messages for our conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction_prompt_template.format(context=context)},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    # apply the chat template and return the sample\n",
    "    # we'll also return the single answer we expect and the list of messages without\n",
    "    # the chat template in case we need them later.\n",
    "    sample_conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": sample_conversation, \"messages\": messages, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd342b13f784c9d9fe847853d9cf334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc3d3f8982d444ba70cd5a6c609097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb97a4402ecd4c0e84b8c8920e06919d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_training_samples = training_samples.map(convert_squad_sample_to_llama_conversation)\n",
    "conversation_test_samples = test_samples.map(convert_squad_sample_to_llama_conversation)\n",
    "conversation_validation_samples = validation_samples.map(convert_squad_sample_to_llama_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "CUDA backend validation successful.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file model.safetensors from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "CUDA backend validation successful.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# to help save on gpu space and run this a bit faster we'll load the model in 4bit\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4262\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# to help save on gpu space and run this a bit faster we'll load the model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "# rank defines the rank of the adapter matrix,\n",
    "# the higher the rank, the more complex the task it's trying to learn\n",
    "rank = 128\n",
    "\n",
    "# the alpha is a scaling factor hyper parameter, basically controls how much our\n",
    "# adapter will influence the models output, the higher this value\n",
    "# the more our adapter will overpower the original model weights.\n",
    "# there is a lot of advice out there for what the alpha value should be\n",
    "# keeping the alpha at around 2x of what the rank is works for this notebook\n",
    "alpha = rank*2\n",
    "peft_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    lora_dropout=0.05, # dropout for the lora layers while training, to avoid overfitting\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # the target modules defines what types of layers to add lora adapters too, so in the network\n",
    "    # any model that have a name in this list will have a lora adapter added to it,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "adamw is not a valid OptimizerNames, please select one of ['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'adamw_torch_4bit', 'adamw_torch_8bit', 'ademamix', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'ademamix_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_ademamix_32bit', 'paged_ademamix_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise', 'lomo', 'adalomo', 'grokadamw', 'schedule_free_radam', 'schedule_free_adamw', 'schedule_free_sgd', 'apollo_adamw', 'apollo_adamw_layerwise']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m model_checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/tiny-llama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# an important note is that the loss function isn't defined here,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# it's instead stored as a model parameter for models in hf,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# in the case of llama it is cross entropy loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# first define some training arguments\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madamw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#specify what optimizer we wwant to use, in this case a 8bit version of adamw with pagination.\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# define the number of samples per training batch\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# define how many steps to accumulate gradients,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdebug\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we'll save a checkpoint every epoch\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# for llm training we want a fairly high learning rate, 1e-4 is a good starting point but it's worth it to play around with this value\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# and set our learning rate decay\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# now that we have our arguments, we'll use that to create our trainer,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# passing in the model, dataset, peft config, tokenizer, ect\u001b[39;00m\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mconversation_training_samples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_arguments\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/training_args.py:1741\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m   1739\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler_type reduce_lr_on_plateau requires torch>=0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m \u001b[43mOptimizerNames\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madafactor:\n\u001b[1;32m   1743\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--adafactor` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--optim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m adafactor` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/enum.py:718\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    717\u001b[0m             exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m ve_exc\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# ensure all variables that could hold an exception are destroyed\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/enum.py:700\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_missing_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    702\u001b[0m     exc \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/miniconda3/envs/moba/lib/python3.10/site-packages/transformers/utils/generic.py:498\u001b[0m, in \u001b[0;36mExplicitEnum._missing_\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_missing_\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, please select one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_value2member_map_\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: adamw is not a valid OptimizerNames, please select one of ['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'adamw_torch_4bit', 'adamw_torch_8bit', 'ademamix', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'ademamix_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_ademamix_32bit', 'paged_ademamix_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise', 'lomo', 'adalomo', 'grokadamw', 'schedule_free_radam', 'schedule_free_adamw', 'schedule_free_sgd', 'apollo_adamw', 'apollo_adamw_layerwise']"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_checkpoint_path = \"./model/llama3.2-1b\"\n",
    "\n",
    "# an important note is that the loss function isn't defined here,\n",
    "# it's instead stored as a model parameter for models in hf,\n",
    "# in the case of llama it is cross entropy loss\n",
    "\n",
    "# first define some training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=model_checkpoint_path,\n",
    "    optim='adafactor', #specify what optimizer we wwant to use, in this case a 8bit version of adamw with pagination.\n",
    "    per_device_train_batch_size=8, # define the number of samples per training batch\n",
    "    gradient_accumulation_steps=4, # define how many steps to accumulate gradients,\n",
    "    log_level='debug',\n",
    "    eval_strategy = \"steps\",\n",
    "    save_strategy='steps', # we'll save a checkpoint every epoch\n",
    "    logging_steps=8,\n",
    "    eval_steps=8,\n",
    "    save_steps=8,\n",
    "    learning_rate=1e-5, # for llm training we want a fairly high learning rate, 1e-4 is a good starting point but it's worth it to play around with this value\n",
    "    fp16=True,\n",
    "    num_train_epochs=4,\n",
    "    max_steps=120,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end = True,\n",
    "    overwrite_output_dir = True,\n",
    "    lr_scheduler_type='linear',# and set our learning rate decay\n",
    ")\n",
    "\n",
    "# now that we have our arguments, we'll use that to create our trainer,\n",
    "# passing in the model, dataset, peft config, tokenizer, ect\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=conversation_training_samples,\n",
    "    eval_dataset=conversation_test_samples,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 100,925,440 || all params: 1,200,973,824 || trainable%: 8.4036\n"
     ]
    }
   ],
   "source": [
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 05:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.672109365463257, 'eval_model_preparation_time': 0.002, 'eval_runtime': 15.6049, 'eval_samples_per_second': 48.062, 'eval_steps_per_second': 6.024}\n"
     ]
    }
   ],
   "source": [
    "initial_eval_values = trainer.evaluate()\n",
    "print(initial_eval_values)\n",
    "initial_eval_loss = initial_eval_values['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 120\n",
      "  Number of trainable parameters = 100,925,440\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 13:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.179600</td>\n",
       "      <td>1.363259</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.239600</td>\n",
       "      <td>1.010158</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.090100</td>\n",
       "      <td>0.984117</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.117500</td>\n",
       "      <td>0.975623</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.058400</td>\n",
       "      <td>0.972669</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.041100</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.022300</td>\n",
       "      <td>0.971979</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>0.970095</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.971000</td>\n",
       "      <td>0.971331</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>0.972696</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.982700</td>\n",
       "      <td>0.971532</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>0.971078</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.967300</td>\n",
       "      <td>0.971125</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.970992</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-8\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-8/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-8/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-16\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-16/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-24\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-24/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-32\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-32/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-40\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-40/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-48\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-48/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-56\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-56/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-56/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-64\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-64/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-72\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-72/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-72/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-80\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-80/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-88\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-88/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-96\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-96/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-104\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-104/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-112\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-112/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-112/special_tokens_map.json\n",
      "/home/hans/miniconda3/envs/moba/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: question, answers, context, title, id, answer, text. If question, answers, context, title, id, answer, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./model/tiny-llama/checkpoint-120\n",
      "loading configuration file config.json from cache at /home/hans/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./model/tiny-llama/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./model/tiny-llama/checkpoint-120/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./model/tiny-llama/checkpoint-72 (score: 0.970095157623291).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=1.1158244331677756, metrics={'train_runtime': 807.982, 'train_samples_per_second': 4.753, 'train_steps_per_second': 0.149, 'total_flos': 1.264995821469696e+16, 'train_loss': 1.1158244331677756})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: context, text, id, question, title, answer, answers. If context, text, id, question, title, answer, answers are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 51:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.13447105884552, 'eval_model_preparation_time': 0.0031, 'eval_runtime': 3123.112, 'eval_samples_per_second': 0.24, 'eval_steps_per_second': 0.03}\n"
     ]
    }
   ],
   "source": [
    "# initial_eval_values = trainer.evaluate()\n",
    "# print(initial_eval_values)\n",
    "# initial_eval_loss = initial_eval_values['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1796, 1.2396, 1.0901, 1.1175, 1.0412, 1.0584, 1.0411, 1.0223, 0.9953, 0.971, 1.038, 0.9827, 1.002, 0.9673, 0.9914]\n",
      "[2.672109365463257, 1.3632590770721436, 1.0101580619812012, 0.9841168522834778, 0.975623369216919, 0.9724697470664978, 0.9726694226264954, 0.9700997471809387, 0.9719794988632202, 0.970095157623291, 0.9713305830955505, 0.9726958870887756, 0.9715316295623779, 0.9710776209831238, 0.9711250066757202, 0.9709919691085815]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss_history = []\n",
    "eval_loss_history = [initial_eval_loss]\n",
    "for step in trainer.state.log_history:\n",
    "  if 'loss' in step:\n",
    "    training_loss_history.append(step['loss'])\n",
    "  elif \"eval_loss\" in step:\n",
    "    eval_loss_history.append(step['eval_loss'])\n",
    "\n",
    "print(training_loss_history)\n",
    "print(eval_loss_history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "time_steps = [i*16 for i in range(1, len(training_loss_history)+1)]\n",
    "plt.plot(time_steps, training_loss_history, label=\"train loss\")\n",
    "plt.plot([0]+time_steps, eval_loss_history, label=\"eval loss\")\n",
    "plt.title(\"Train and Eval Loss During Training\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./results/llama3.2-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "trained_adapter_dir = modelpath  # your checkpoint folder clearly stated here\n",
    "\n",
    "# Load base tokenizer explicitly\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Clearly load base model explicitly\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Explicitly load your trained LoRA adapters clearly\n",
    "model = PeftModel.from_pretrained(base_model, trained_adapter_dir)\n",
    "\n",
    "# Set tokenizer explicitly\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Use model explicitly for inference clearly\n",
    "prompt = \"When did Beyonce start becoming popular?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=256)\n",
    "\n",
    "response = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
