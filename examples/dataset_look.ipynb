{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
       " 'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = ds[\"train\"]\n",
    "raw_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6775b7802143ce8753f78718bfd792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f889f7aca0d4d2ba8d70a90ea92b0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/487k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dc3911692842559317839770a4e16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f9224f65d74934b7f505ed58742417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"socratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = ds[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hans\\.conda\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler, DataCollatorWithPadding\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'user' %}[USER]: {{ message['content'] }}{% elif message['role'] == 'assistant' %}[ASSISTANT]: {{message['content']}}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}\"\n",
    "    \"{{ 'User: ' + message['content'] }}\"\n",
    "    \"{% elif message['role'] == 'assistant' %}\"\n",
    "    \"{{ 'Assistant: ' + message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{{ eos_token }}\"\n",
    ")\n",
    "tokenizer.chat_template = chat_template\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(samples):\n",
    "    conversations = []\n",
    "\n",
    "    for question, answer in zip(samples['question'], samples['answer']):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        conversation = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False,\n",
    "            template=chat_template    \n",
    "        )\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    tokenized_output = tokenizer(\n",
    "        conversations, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "print(raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:02<00:00, 3492.02 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 3879.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Correctly map in batch mode\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a module script at d:\\LLMResearch\\MoBA\\examples\\openai\\gsm8k\\gsm8k.py. Module 'openai/gsm8k' doesn't exist on the Hugging Face Hub either.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m metric = \u001b[43mevaluate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai/gsm8k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model.eval()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m eval_dl:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hans\\.conda\\envs\\llm\\Lib\\site-packages\\evaluate\\loading.py:748\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[32m    704\u001b[39m \n\u001b[32m    705\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    745\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    746\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    747\u001b[39m download_mode = DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode.REUSE_DATASET_IF_EXISTS)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m evaluation_module = \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m evaluation_cls = import_main_class(evaluation_module.module_path)\n\u001b[32m    752\u001b[39m evaluation_instance = evaluation_cls(\n\u001b[32m    753\u001b[39m     config_name=config_name,\n\u001b[32m    754\u001b[39m     process_id=process_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    760\u001b[39m     **init_kwargs,\n\u001b[32m    761\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hans\\.conda\\envs\\llm\\Lib\\site-packages\\evaluate\\loading.py:681\u001b[39m, in \u001b[36mevaluation_module_factory\u001b[39m\u001b[34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[39m\n\u001b[32m    679\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[32m    680\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    682\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    683\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hugging Face Hub either.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    684\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Couldn't find a module script at d:\\LLMResearch\\MoBA\\examples\\openai\\gsm8k\\gsm8k.py. Module 'openai/gsm8k' doesn't exist on the Hugging Face Hub either."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"openai/gsm8k\", \"main\")\n",
    "model.eval()\n",
    "for batch in eval_dl:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "evaluate.load(\n",
      "    path: str,\n",
      "    config_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    module_type: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    process_id: int = \u001b[32m0\u001b[39m,\n",
      "    num_process: int = \u001b[32m1\u001b[39m,\n",
      "    cache_dir: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    experiment_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_in_memory: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    download_config: Optional[datasets.download.download_config.DownloadConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    download_mode: Optional[datasets.download.download_manager.DownloadMode] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    revision: Union[str, datasets.utils.version.Version, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    **init_kwargs,\n",
      ") -> evaluate.module.EvaluationModule\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "\u001b[38;5;28;01mdef\u001b[39;00m load(\n",
      "    path: str,\n",
      "    config_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    module_type: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    process_id: int = \u001b[32m0\u001b[39m,\n",
      "    num_process: int = \u001b[32m1\u001b[39m,\n",
      "    cache_dir: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    experiment_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_in_memory: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    download_config: Optional[DownloadConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    download_mode: Optional[DownloadMode] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    revision: Optional[Union[str, Version]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    **init_kwargs,\n",
      ") -> EvaluationModule:\n",
      "    \u001b[33m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39m\n",
      "\n",
      "\u001b[33m    Args:\u001b[39m\n",
      "\n",
      "\u001b[33m        path (`str`):\u001b[39m\n",
      "\u001b[33m            Path to the evaluation processing script with the evaluation builder. Can be either:\u001b[39m\n",
      "\u001b[33m                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\u001b[39m\n",
      "\u001b[33m                    e.g. `'./metrics/rouge'` or `'./metrics/rouge/rouge.py'`\u001b[39m\n",
      "\u001b[33m                - a evaluation module identifier on the HuggingFace evaluate repo e.g. `'rouge'` or `'bleu'` that are in either `'metrics/'`,\u001b[39m\n",
      "\u001b[33m                    `'comparisons/'`, or `'measurements/'` depending on the provided `module_type`\u001b[39m\n",
      "\u001b[33m        config_name (`str`, *optional*):\u001b[39m\n",
      "\u001b[33m            Selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset).\u001b[39m\n",
      "\u001b[33m        module_type (`str`, default `'metric'`):\u001b[39m\n",
      "\u001b[33m            Type of evaluation module, can be one of `'metric'`, `'comparison'`, or `'measurement'`.\u001b[39m\n",
      "\u001b[33m        process_id (`int`, *optional*):\u001b[39m\n",
      "\u001b[33m            For distributed evaluation: id of the process.\u001b[39m\n",
      "\u001b[33m        num_process (`int`, *optional*):\u001b[39m\n",
      "\u001b[33m            For distributed evaluation: total number of processes.\u001b[39m\n",
      "\u001b[33m        cache_dir (`str`, *optional*):\u001b[39m\n",
      "\u001b[33m            Path to store the temporary predictions and references (default to `~/.cache/huggingface/evaluate/`).\u001b[39m\n",
      "\u001b[33m        experiment_id (`str`):\u001b[39m\n",
      "\u001b[33m            A specific experiment id. This is used if several distributed evaluations share the same file system.\u001b[39m\n",
      "\u001b[33m            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\u001b[39m\n",
      "\u001b[33m        keep_in_memory (`bool`):\u001b[39m\n",
      "\u001b[33m            Whether to store the temporary results in memory (defaults to `False`).\u001b[39m\n",
      "\u001b[33m        download_config ([`~evaluate.DownloadConfig`], *optional*):\u001b[39m\n",
      "\u001b[33m            Specific download configuration parameters.\u001b[39m\n",
      "\u001b[33m        download_mode ([`DownloadMode`], defaults to `REUSE_DATASET_IF_EXISTS`):\u001b[39m\n",
      "\u001b[33m            Download/generate mode.\u001b[39m\n",
      "\u001b[33m        revision (`Union[str, evaluate.Version]`, *optional*):\u001b[39m\n",
      "\u001b[33m            If specified, the module will be loaded from the datasets repository\u001b[39m\n",
      "\u001b[33m            at this version. By default it is set to the local version of the lib. Specifying a version that is different from\u001b[39m\n",
      "\u001b[33m            your local version of the lib might cause compatibility issues.\u001b[39m\n",
      "\n",
      "\u001b[33m    Returns:\u001b[39m\n",
      "\u001b[33m        [`evaluate.EvaluationModule`]\u001b[39m\n",
      "\n",
      "\u001b[33m    Example:\u001b[39m\n",
      "\n",
      "\u001b[33m        ```py\u001b[39m\n",
      "\u001b[33m        >>> from evaluate import load\u001b[39m\n",
      "\u001b[33m        >>> accuracy = load(\"accuracy\")\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    download_mode = DownloadMode(download_mode \u001b[38;5;28;01mor\u001b[39;00m DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
      "    evaluation_module = evaluation_module_factory(\n",
      "        path, module_type=module_type, revision=revision, download_config=download_config, download_mode=download_mode\n",
      "    )\n",
      "    evaluation_cls = import_main_class(evaluation_module.module_path)\n",
      "    evaluation_instance = evaluation_cls(\n",
      "        config_name=config_name,\n",
      "        process_id=process_id,\n",
      "        num_process=num_process,\n",
      "        cache_dir=cache_dir,\n",
      "        keep_in_memory=keep_in_memory,\n",
      "        experiment_id=experiment_id,\n",
      "        hash=evaluation_module.hash,\n",
      "        **init_kwargs,\n",
      "    )\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;28;01mand\u001b[39;00m module_type != evaluation_instance.module_type:\n",
      "        \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
      "            f\"No module of module type '{module_type}' not found for '{path}' locally, or on the Hugging Face Hub. Found module of module type '{evaluation_instance.module_type}' instead.\"\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;66;03m# Download and prepare resources for the metric\u001b[39;00m\n",
      "    evaluation_instance.download_and_prepare(download_config=download_config)\n",
      "\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_instance\n",
      "\u001b[31mFile:\u001b[39m      c:\\users\\hans\\.conda\\envs\\llm\\lib\\site-packages\\evaluate\\loading.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "evaluate.load??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
